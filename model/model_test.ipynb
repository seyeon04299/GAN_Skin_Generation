{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision import models\n",
    "import sys\n",
    "\n",
    "## Factors for discriminator and generator channel changes\n",
    "factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]\n",
    "\n",
    "\n",
    "class WSConv2d(nn.Module):          #Weighted Scaled convolutional layers\n",
    "    '''\n",
    "    Weighted scaled Conv2d (Equalized Learning Rate - Section 4.1 of Notes)\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1,gain=2):\n",
    "        super(WSConv2d,self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain/(in_channels*(kernel_size**2)))**0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.conv(x*self.scale) + self.bias.view(1,self.bias.shape[0],1,1)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, norm_layer='PixelNorm', use_pixelnorm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.use_pn = use_pixelnorm\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
    "\n",
    "\n",
    "class Generator_ProGAN(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3, norm_layer = 'PixelNorm', factors =[]):\n",
    "        super(Generator_ProGAN, self).__init__()\n",
    "        # self.norm_layer = getattr(module_norm, norm_layer)/\n",
    "        # initial takes 1x1 -> 4x4\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        )\n",
    "\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            in_channels, img_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.prog_blocks, self.rgb_layers = (\n",
    "            nn.ModuleList([]),\n",
    "            nn.ModuleList([self.initial_rgb]),\n",
    "        )\n",
    "\n",
    "        for i in range(\n",
    "            len(factors) - 1\n",
    "        ):  # -1 to prevent index error because of factors[i+1]\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            print('i : ', i, ' | conv_in_c : ', conv_in_c)\n",
    "            conv_out_c = int(in_channels * factors[i + 1])\n",
    "            print('i : ', i, ' | conv_out_c : ', conv_out_c)\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "    def forward(self, x, alpha, steps):     # steps=0 (4x4), steps=1 (8x8), ...\n",
    "        print('steps : ', steps)\n",
    "        print('x.shape : ', x.shape)\n",
    "        out = self.initial(x)   # 4x4\n",
    "        print('out.shape : ', out.shape)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "\n",
    "        for step in range(steps):\n",
    "            print('step:',step)\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            print('upscaled : ', upscaled.shape)\n",
    "            # print(self.prog_blocks[step])\n",
    "            \n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "            print('out.shape : ', out.shape)\n",
    "\n",
    "        # The number of channels in upscale will stay the same, while\n",
    "        # out which has moved through prog_blocks might change. To ensure\n",
    "        # we can convert both to rgb we use different rgb_layers\n",
    "        # (steps-1) and steps for upscaled, out respectively\n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
    "        print('final_upscaled : ', final_upscaled.shape)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        print('final_out : ', final_out.shape)\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator_ProGAN(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3, factors =[]):\n",
    "        super(Discriminator_ProGAN,self).__init__()\n",
    "\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        # Work Backwords form factor\n",
    "        for i in range(len(factors)-1, 0,-1):\n",
    "            conv_in = int(in_channels*factors[i])\n",
    "            conv_out = int(in_channels*factors[i-1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in,conv_out,use_pixelnorm=False))\n",
    "            self.rgb_layers.append(WSConv2d(img_channels,conv_in,kernel_size=1,stride=1,padding=0))\n",
    "        \n",
    "        \n",
    "        # RGB layer for 4x4 input size, to \"mirror\" the generator initial_rgb\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "\n",
    "        # DownSampling\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Block for 4x4 input size\n",
    "        self.final_block = nn.Sequential(\n",
    "            # +1 to in_channels because we concatenate from MiniBatch std\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(\n",
    "                in_channels, 1, kernel_size=1, padding=0, stride=1\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        batch_statistics = (\n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        )\n",
    "        # we take the std for each example (across all channels, and pixels) then we repeat it\n",
    "        # for a single channel and concatenate it with the image. In this way the discriminator\n",
    "        # will get information about the variation in the batch/image\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        print('Discriminatpor forward')\n",
    "        cur_step = len(self.prog_blocks)-steps\n",
    "        print(cur_step)\n",
    "        # Convert from rgb as initial step\n",
    "        print('x: ', x.shape)\n",
    "        print(self.rgb_layers[cur_step])\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "        \n",
    "        print('OUT: ', out.shape)\n",
    "\n",
    "        if steps==0:\n",
    "            out = self.minibatch_std(out)\n",
    "            print('step0 out : ', out.shape)\n",
    "            out = self.final_block(out)\n",
    "            print('step0 out : ', out.shape)\n",
    "            return out.view(out.shape[0],-1)\n",
    "\n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step+1](self.avg_pool(x)))\n",
    "        print('downscaled ; ', downscaled.shape)\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "        print('out ; ', out.shape)\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "        print('out ; ', out.shape)\n",
    "\n",
    "        for step in range(cur_step+1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            print('out ; ', out.shape)\n",
    "            out = self.avg_pool(out)\n",
    "            print('out ; ', out.shape)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        print('out ; ', out.shape)\n",
    "        return self.final_block(out).view(out.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i :  0  | conv_in_c :  256\n",
      "i :  0  | conv_out_c :  256\n",
      "i :  1  | conv_in_c :  256\n",
      "i :  1  | conv_out_c :  256\n",
      "i :  2  | conv_in_c :  256\n",
      "i :  2  | conv_out_c :  256\n",
      "i :  3  | conv_in_c :  256\n",
      "i :  3  | conv_out_c :  128\n",
      "i :  4  | conv_in_c :  128\n",
      "i :  4  | conv_out_c :  64\n",
      "i :  5  | conv_in_c :  64\n",
      "i :  5  | conv_out_c :  32\n",
      "i :  6  | conv_in_c :  32\n",
      "i :  6  | conv_out_c :  16\n",
      "i :  7  | conv_in_c :  16\n",
      "i :  7  | conv_out_c :  8\n"
     ]
    }
   ],
   "source": [
    "Z_DIM = 100\n",
    "IN_CHANNELS = 256\n",
    "gen = Generator_ProGAN(Z_DIM, IN_CHANNELS, img_channels=3,factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32])\n",
    "critic = Discriminator_ProGAN(Z_DIM, IN_CHANNELS, img_channels=3,factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32])\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4\n",
      "1 8\n",
      "2 16\n",
      "3 32\n",
      "4 64\n",
      "5 128\n",
      "6 256\n",
      "7 512\n",
      "8 1024\n"
     ]
    }
   ],
   "source": [
    "for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "    num_steps = int(log2(img_size / 4))\n",
    "    print(num_steps, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator_ProGAN(\n",
       "  (prog_blocks): ModuleList(\n",
       "    (0): ConvBlock(\n",
       "      (conv1): WSConv2d(\n",
       "        (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): WSConv2d(\n",
       "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (leaky): LeakyReLU(negative_slope=0.2)\n",
       "      (pn): PixelNorm()\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (conv1): WSConv2d(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): WSConv2d(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (leaky): LeakyReLU(negative_slope=0.2)\n",
       "      (pn): PixelNorm()\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (conv1): WSConv2d(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): WSConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (leaky): LeakyReLU(negative_slope=0.2)\n",
       "      (pn): PixelNorm()\n",
       "    )\n",
       "    (3): ConvBlock(\n",
       "      (conv1): WSConv2d(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): WSConv2d(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (leaky): LeakyReLU(negative_slope=0.2)\n",
       "      (pn): PixelNorm()\n",
       "    )\n",
       "    (4): ConvBlock(\n",
       "      (conv1): WSConv2d(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): WSConv2d(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (leaky): LeakyReLU(negative_slope=0.2)\n",
       "      (pn): PixelNorm()\n",
       "    )\n",
       "    (5): ConvBlock(\n",
       "      (conv1): WSConv2d(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): WSConv2d(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (leaky): LeakyReLU(negative_slope=0.2)\n",
       "      (pn): PixelNorm()\n",
       "    )\n",
       "    (6): ConvBlock(\n",
       "      (conv1): WSConv2d(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): WSConv2d(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (leaky): LeakyReLU(negative_slope=0.2)\n",
       "      (pn): PixelNorm()\n",
       "    )\n",
       "    (7): ConvBlock(\n",
       "      (conv1): WSConv2d(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): WSConv2d(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (leaky): LeakyReLU(negative_slope=0.2)\n",
       "      (pn): PixelNorm()\n",
       "    )\n",
       "  )\n",
       "  (rgb_layers): ModuleList(\n",
       "    (0): WSConv2d(\n",
       "      (conv): Conv2d(3, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): WSConv2d(\n",
       "      (conv): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (2): WSConv2d(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (3): WSConv2d(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (4): WSConv2d(\n",
       "      (conv): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (5): WSConv2d(\n",
       "      (conv): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (6): WSConv2d(\n",
       "      (conv): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (7): WSConv2d(\n",
       "      (conv): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (8): WSConv2d(\n",
       "      (conv): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (leaky): LeakyReLU(negative_slope=0.2)\n",
       "  (initial_rgb): WSConv2d(\n",
       "    (conv): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (final_block): Sequential(\n",
       "    (0): WSConv2d(\n",
       "      (conv): Conv2d(257, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): WSConv2d(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    )\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): WSConv2d(\n",
       "      (conv): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps :  0\n",
      "x.shape :  torch.Size([1, 100, 1, 1])\n",
      "out.shape :  torch.Size([1, 256, 4, 4])\n",
      "torch.Size([1, 3, 4, 4])\n",
      "Discriminatpor forward\n",
      "8\n",
      "x:  torch.Size([1, 3, 4, 4])\n",
      "WSConv2d(\n",
      "  (conv): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      ")\n",
      "OUT:  torch.Size([1, 256, 4, 4])\n",
      "step0 out :  torch.Size([1, 257, 4, 4])\n",
      "step0 out :  torch.Size([1, 1, 1, 1])\n",
      "Success! At img size: 4\n"
     ]
    }
   ],
   "source": [
    "num_steps = 0\n",
    "img_size = 4\n",
    "x = torch.randn((1, Z_DIM, 1, 1))\n",
    "z = gen(x, 0.5, steps=num_steps)\n",
    "print(z.shape)\n",
    "assert z.shape == (1, 3, img_size, img_size)\n",
    "out = critic(z, alpha=0.5, steps=num_steps)\n",
    "assert out.shape == (1, 1)\n",
    "print(f\"Success! At img size: {img_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_obj(self, name, module, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Finds a function handle with the name given as 'type' in config, and returns the\n",
    "    instance initialized with corresponding arguments given.\n",
    "\n",
    "    `object = config.init_obj('name', module, a, b=1)`\n",
    "    is equivalent to\n",
    "    `object = module.name(a, b=1)`\n",
    "    \"\"\"\n",
    "    print(module)\n",
    "    module_name = self[name]['type']\n",
    "    print('module name : ',module_name)\n",
    "    module_args = dict(self[name]['args'])\n",
    "\n",
    "    ## Change 'Beta' type to TUPLE for ADAM OPTIMIZER\n",
    "    if 'optimizer' in module_name and 'betas' in module_args.keys():\n",
    "        module_args['betas'] = tuple(module_args['betas'])\n",
    "\n",
    "    print(module_args)\n",
    "    assert all([k not in module_args for k in kwargs]), 'Overwriting kwargs given in config file is not allowed'\n",
    "    module_args.update(kwargs)\n",
    "    print(module_args)\n",
    "    return getattr(module, module_name)(*args, **module_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-ccf7d14ca2f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNUskinDataLoader_ProGAN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_loader'"
     ]
    }
   ],
   "source": [
    "from data_loader import KNUskinDataLoader_ProGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "block id 0\n",
      "kernel_size =  3\n",
      "inplanes =  64\n",
      "outplanes =  64\n",
      "stride =  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "block id 1\n",
      "kernel_size =  3\n",
      "inplanes =  64\n",
      "outplanes =  128\n",
      "stride =  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "block id 2\n",
      "kernel_size =  3\n",
      "inplanes =  128\n",
      "outplanes =  256\n",
      "stride =  2\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "block id 3\n",
      "kernel_size =  3\n",
      "inplanes =  256\n",
      "outplanes =  512\n",
      "stride =  2\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ResNet2d(ResBlk2d, inplanes= 64, kernel_size=3, stride=2, num_classes=2, dropout=0.5).to(device)\n",
    "x = torch.randn(3, 3, 224, 224).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  torch.Size([3, 3, 224, 224]) aux :  torch.Size([3, 150528])\n",
      "conv1 :  torch.Size([3, 64, 112, 112])\n",
      "-------------------\n",
      "layer0 torch.Size([3, 64, 56, 56])\n",
      "identity =  torch.Size([3, 64, 56, 56])\n",
      "X_shape =  torch.Size([3, 64, 56, 56])\n",
      "conv1 shape =  torch.Size([3, 64, 28, 28])\n",
      "conv2 shape =  torch.Size([3, 64, 28, 28])\n",
      " out =  torch.Size([3, 64, 28, 28])\n",
      "downsample =  Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "identity2 =  torch.Size([3, 64, 28, 28])\n",
      "torch.Size([3, 64, 28, 28])\n",
      "dropout out =  torch.Size([3, 64, 28, 28])\n",
      "identity =  torch.Size([3, 64, 28, 28])\n",
      "X_shape =  torch.Size([3, 64, 28, 28])\n",
      "conv1 shape =  torch.Size([3, 64, 28, 28])\n",
      "conv2 shape =  torch.Size([3, 64, 28, 28])\n",
      " out =  torch.Size([3, 64, 28, 28])\n",
      "downsample =  None\n",
      "identity2 =  torch.Size([3, 64, 28, 28])\n",
      "torch.Size([3, 64, 28, 28])\n",
      "dropout out =  torch.Size([3, 64, 28, 28])\n",
      "-------------------\n",
      "layer1 torch.Size([3, 64, 28, 28])\n",
      "identity =  torch.Size([3, 64, 28, 28])\n",
      "X_shape =  torch.Size([3, 64, 28, 28])\n",
      "conv1 shape =  torch.Size([3, 128, 14, 14])\n",
      "conv2 shape =  torch.Size([3, 128, 14, 14])\n",
      " out =  torch.Size([3, 128, 14, 14])\n",
      "downsample =  Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "identity2 =  torch.Size([3, 128, 14, 14])\n",
      "torch.Size([3, 128, 14, 14])\n",
      "dropout out =  torch.Size([3, 128, 14, 14])\n",
      "identity =  torch.Size([3, 128, 7, 7])\n",
      "X_shape =  torch.Size([3, 128, 7, 7])\n",
      "conv1 shape =  torch.Size([3, 128, 7, 7])\n",
      "conv2 shape =  torch.Size([3, 128, 7, 7])\n",
      " out =  torch.Size([3, 128, 7, 7])\n",
      "downsample =  None\n",
      "identity2 =  torch.Size([3, 128, 7, 7])\n",
      "torch.Size([3, 128, 7, 7])\n",
      "dropout out =  torch.Size([3, 128, 7, 7])\n",
      "-------------------\n",
      "layer2 torch.Size([3, 128, 4, 4])\n",
      "identity =  torch.Size([3, 128, 4, 4])\n",
      "X_shape =  torch.Size([3, 128, 4, 4])\n",
      "conv1 shape =  torch.Size([3, 256, 2, 2])\n",
      "conv2 shape =  torch.Size([3, 256, 2, 2])\n",
      " out =  torch.Size([3, 256, 2, 2])\n",
      "downsample =  Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "identity2 =  torch.Size([3, 256, 2, 2])\n",
      "torch.Size([3, 256, 2, 2])\n",
      "dropout out =  torch.Size([3, 256, 2, 2])\n",
      "identity =  torch.Size([3, 256, 2, 2])\n",
      "X_shape =  torch.Size([3, 256, 2, 2])\n",
      "conv1 shape =  torch.Size([3, 256, 2, 2])\n",
      "conv2 shape =  torch.Size([3, 256, 2, 2])\n",
      " out =  torch.Size([3, 256, 2, 2])\n",
      "downsample =  None\n",
      "identity2 =  torch.Size([3, 256, 2, 2])\n",
      "torch.Size([3, 256, 2, 2])\n",
      "dropout out =  torch.Size([3, 256, 2, 2])\n",
      "-------------------\n",
      "layer3 torch.Size([3, 256, 2, 2])\n",
      "identity =  torch.Size([3, 256, 2, 2])\n",
      "X_shape =  torch.Size([3, 256, 2, 2])\n",
      "conv1 shape =  torch.Size([3, 512, 1, 1])\n",
      "conv2 shape =  torch.Size([3, 512, 1, 1])\n",
      " out =  torch.Size([3, 512, 1, 1])\n",
      "downsample =  Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "identity2 =  torch.Size([3, 512, 1, 1])\n",
      "torch.Size([3, 512, 1, 1])\n",
      "dropout out =  torch.Size([3, 512, 1, 1])\n",
      "identity =  torch.Size([3, 512, 1, 1])\n",
      "X_shape =  torch.Size([3, 512, 1, 1])\n",
      "conv1 shape =  torch.Size([3, 512, 1, 1])\n",
      "conv2 shape =  torch.Size([3, 512, 1, 1])\n",
      " out =  torch.Size([3, 512, 1, 1])\n",
      "downsample =  None\n",
      "identity2 =  torch.Size([3, 512, 1, 1])\n",
      "torch.Size([3, 512, 1, 1])\n",
      "dropout out =  torch.Size([3, 512, 1, 1])\n",
      "-------------------\n",
      "layer4 torch.Size([3, 512, 1, 1])\n",
      "-------------------\n",
      "flatten torch.Size([3, 512])\n",
      "final torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc7f54debfa0ced2642815502d19580d9cc5faeb3690fcabf793ce32f40109c8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
