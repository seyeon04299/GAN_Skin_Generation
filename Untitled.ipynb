{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hearing-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import torch\n",
    "import numpy as np\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.loss as module_loss\n",
    "import model.metric as module_metric\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from trainer import Trainer\n",
    "from utils import prepare_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    base_dir = os.path.join('C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/HAM_TEST_Project/data/HAM10000/' )\n",
    "    all_image_path = glob(os.path.join(base_dir, '*', '*.jpg'))\n",
    "    imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "def get_data(base_dir, imageid_path_dict):\n",
    "    \n",
    "    lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'dermatofibroma',\n",
    "    'bkl': 'Benign keratosis-like lesions ',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "    }\n",
    "\n",
    "    df_original = pd.read_csv(os.path.join(base_dir, 'HAM10000_metadata.csv'))\n",
    "    df_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\n",
    "    df_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\n",
    "    df_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\n",
    "    # df_original.head()\n",
    "\n",
    "    df_original[['cell_type_idx', 'cell_type']].sort_values('cell_type_idx').drop_duplicates()\n",
    "\n",
    "    # this will tell us how many images are associated with each lesion_id\n",
    "    df_undup = df_original.groupby('lesion_id').count()\n",
    "    # now we filter out lesion_id's that have only one image associated with it\n",
    "    df_undup = df_undup[df_undup['image_id'] == 1]\n",
    "    df_undup.reset_index(inplace=True)\n",
    "\n",
    "    # here we identify lesion_id's that have duplicate images and those that have only one image.\n",
    "    def get_duplicates(x):\n",
    "        unique_list = list(df_undup['lesion_id'])\n",
    "        if x in unique_list:\n",
    "            return 'unduplicated'\n",
    "        else:\n",
    "            return 'duplicated'\n",
    "\n",
    "    # create a new colum that is a copy of the lesion_id column\n",
    "    df_original['duplicates'] = df_original['lesion_id']\n",
    "\n",
    "    # apply the function to this new column\n",
    "    df_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)\n",
    "\n",
    "    # now we filter out images that don't have duplicates\n",
    "    df_undup = df_original[df_original['duplicates'] == 'unduplicated']\n",
    "    return df_undup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         lesion_id      image_id     dx dx_type   age     sex localization  \\\n",
       "0      HAM_0000118  ISIC_0027419    bkl   histo  80.0    male        scalp   \n",
       "1      HAM_0000118  ISIC_0025030    bkl   histo  80.0    male        scalp   \n",
       "2      HAM_0002730  ISIC_0026769    bkl   histo  80.0    male        scalp   \n",
       "3      HAM_0002730  ISIC_0025661    bkl   histo  80.0    male        scalp   \n",
       "4      HAM_0001466  ISIC_0031633    bkl   histo  75.0    male          ear   \n",
       "...            ...           ...    ...     ...   ...     ...          ...   \n",
       "10010  HAM_0002867  ISIC_0033084  akiec   histo  40.0    male      abdomen   \n",
       "10011  HAM_0002867  ISIC_0033550  akiec   histo  40.0    male      abdomen   \n",
       "10012  HAM_0002867  ISIC_0033536  akiec   histo  40.0    male      abdomen   \n",
       "10013  HAM_0000239  ISIC_0032854  akiec   histo  80.0    male         face   \n",
       "10014  HAM_0003521  ISIC_0032258    mel   histo  70.0  female         back   \n",
       "\n",
       "            dataset  \n",
       "0      vidir_modern  \n",
       "1      vidir_modern  \n",
       "2      vidir_modern  \n",
       "3      vidir_modern  \n",
       "4      vidir_modern  \n",
       "...             ...  \n",
       "10010  vidir_modern  \n",
       "10011  vidir_modern  \n",
       "10012  vidir_modern  \n",
       "10013  vidir_modern  \n",
       "10014  vidir_modern  \n",
       "\n",
       "[10015 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0027419</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>vidir_modern</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0000118</td>\n      <td>ISIC_0025030</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>vidir_modern</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0026769</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>vidir_modern</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002730</td>\n      <td>ISIC_0025661</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>scalp</td>\n      <td>vidir_modern</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0001466</td>\n      <td>ISIC_0031633</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>75.0</td>\n      <td>male</td>\n      <td>ear</td>\n      <td>vidir_modern</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10010</th>\n      <td>HAM_0002867</td>\n      <td>ISIC_0033084</td>\n      <td>akiec</td>\n      <td>histo</td>\n      <td>40.0</td>\n      <td>male</td>\n      <td>abdomen</td>\n      <td>vidir_modern</td>\n    </tr>\n    <tr>\n      <th>10011</th>\n      <td>HAM_0002867</td>\n      <td>ISIC_0033550</td>\n      <td>akiec</td>\n      <td>histo</td>\n      <td>40.0</td>\n      <td>male</td>\n      <td>abdomen</td>\n      <td>vidir_modern</td>\n    </tr>\n    <tr>\n      <th>10012</th>\n      <td>HAM_0002867</td>\n      <td>ISIC_0033536</td>\n      <td>akiec</td>\n      <td>histo</td>\n      <td>40.0</td>\n      <td>male</td>\n      <td>abdomen</td>\n      <td>vidir_modern</td>\n    </tr>\n    <tr>\n      <th>10013</th>\n      <td>HAM_0000239</td>\n      <td>ISIC_0032854</td>\n      <td>akiec</td>\n      <td>histo</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>face</td>\n      <td>vidir_modern</td>\n    </tr>\n    <tr>\n      <th>10014</th>\n      <td>HAM_0003521</td>\n      <td>ISIC_0032258</td>\n      <td>mel</td>\n      <td>histo</td>\n      <td>70.0</td>\n      <td>female</td>\n      <td>back</td>\n      <td>vidir_modern</td>\n    </tr>\n  </tbody>\n</table>\n<p>10015 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df_original = pd.read_csv(os.path.join(base_dir, 'HAM10000_metadata.csv'))\n",
    "df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_undup = get_data(base_dir, imageid_path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        lesion_id      image_id   dx    dx_type   age     sex  \\\n",
       "0     HAM_0001571  ISIC_0029156   nv  follow_up  55.0  female   \n",
       "1     HAM_0003545  ISIC_0029718   nv  follow_up  80.0    male   \n",
       "2     HAM_0005037  ISIC_0028108   nv  follow_up  45.0    male   \n",
       "3     HAM_0002045  ISIC_0029973   df  consensus  60.0    male   \n",
       "4     HAM_0004736  ISIC_0031990   nv  follow_up  55.0    male   \n",
       "...           ...           ...  ...        ...   ...     ...   \n",
       "5509  HAM_0004547  ISIC_0031303  mel      histo  65.0    male   \n",
       "5510  HAM_0006996  ISIC_0027610   nv  follow_up  55.0    male   \n",
       "5511  HAM_0001728  ISIC_0033539  bkl      histo  60.0    male   \n",
       "5512  HAM_0004921  ISIC_0025389  bkl      histo  65.0    male   \n",
       "5513  HAM_0002871  ISIC_0027044   df  consensus  35.0  female   \n",
       "\n",
       "         localization        dataset  \\\n",
       "0     lower extremity  vidir_molemax   \n",
       "1               trunk  vidir_molemax   \n",
       "2     lower extremity  vidir_molemax   \n",
       "3     lower extremity  vidir_molemax   \n",
       "4     lower extremity  vidir_molemax   \n",
       "...               ...            ...   \n",
       "5509             face      rosendahl   \n",
       "5510             back  vidir_molemax   \n",
       "5511             back   vidir_modern   \n",
       "5512             neck      rosendahl   \n",
       "5513  lower extremity  vidir_molemax   \n",
       "\n",
       "                                                   path  \\\n",
       "0     C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...   \n",
       "1     C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...   \n",
       "2     C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...   \n",
       "3     C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...   \n",
       "4     C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...   \n",
       "...                                                 ...   \n",
       "5509  C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...   \n",
       "5510  C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...   \n",
       "5511  C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...   \n",
       "5512  C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...   \n",
       "5513  C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...   \n",
       "\n",
       "                           cell_type  label    duplicates  \n",
       "0                   Melanocytic nevi      4  unduplicated  \n",
       "1                   Melanocytic nevi      4  unduplicated  \n",
       "2                   Melanocytic nevi      4  unduplicated  \n",
       "3                     Dermatofibroma      3  unduplicated  \n",
       "4                   Melanocytic nevi      4  unduplicated  \n",
       "...                              ...    ...           ...  \n",
       "5509                  dermatofibroma      6  unduplicated  \n",
       "5510                Melanocytic nevi      4  unduplicated  \n",
       "5511  Benign keratosis-like lesions       2  unduplicated  \n",
       "5512  Benign keratosis-like lesions       2  unduplicated  \n",
       "5513                  Dermatofibroma      3  unduplicated  \n",
       "\n",
       "[5514 rows x 12 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lesion_id</th>\n      <th>image_id</th>\n      <th>dx</th>\n      <th>dx_type</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>localization</th>\n      <th>dataset</th>\n      <th>path</th>\n      <th>cell_type</th>\n      <th>label</th>\n      <th>duplicates</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HAM_0001571</td>\n      <td>ISIC_0029156</td>\n      <td>nv</td>\n      <td>follow_up</td>\n      <td>55.0</td>\n      <td>female</td>\n      <td>lower extremity</td>\n      <td>vidir_molemax</td>\n      <td>C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...</td>\n      <td>Melanocytic nevi</td>\n      <td>4</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HAM_0003545</td>\n      <td>ISIC_0029718</td>\n      <td>nv</td>\n      <td>follow_up</td>\n      <td>80.0</td>\n      <td>male</td>\n      <td>trunk</td>\n      <td>vidir_molemax</td>\n      <td>C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...</td>\n      <td>Melanocytic nevi</td>\n      <td>4</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HAM_0005037</td>\n      <td>ISIC_0028108</td>\n      <td>nv</td>\n      <td>follow_up</td>\n      <td>45.0</td>\n      <td>male</td>\n      <td>lower extremity</td>\n      <td>vidir_molemax</td>\n      <td>C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...</td>\n      <td>Melanocytic nevi</td>\n      <td>4</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HAM_0002045</td>\n      <td>ISIC_0029973</td>\n      <td>df</td>\n      <td>consensus</td>\n      <td>60.0</td>\n      <td>male</td>\n      <td>lower extremity</td>\n      <td>vidir_molemax</td>\n      <td>C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...</td>\n      <td>Dermatofibroma</td>\n      <td>3</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HAM_0004736</td>\n      <td>ISIC_0031990</td>\n      <td>nv</td>\n      <td>follow_up</td>\n      <td>55.0</td>\n      <td>male</td>\n      <td>lower extremity</td>\n      <td>vidir_molemax</td>\n      <td>C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...</td>\n      <td>Melanocytic nevi</td>\n      <td>4</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5509</th>\n      <td>HAM_0004547</td>\n      <td>ISIC_0031303</td>\n      <td>mel</td>\n      <td>histo</td>\n      <td>65.0</td>\n      <td>male</td>\n      <td>face</td>\n      <td>rosendahl</td>\n      <td>C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...</td>\n      <td>dermatofibroma</td>\n      <td>6</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>5510</th>\n      <td>HAM_0006996</td>\n      <td>ISIC_0027610</td>\n      <td>nv</td>\n      <td>follow_up</td>\n      <td>55.0</td>\n      <td>male</td>\n      <td>back</td>\n      <td>vidir_molemax</td>\n      <td>C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...</td>\n      <td>Melanocytic nevi</td>\n      <td>4</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>5511</th>\n      <td>HAM_0001728</td>\n      <td>ISIC_0033539</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>60.0</td>\n      <td>male</td>\n      <td>back</td>\n      <td>vidir_modern</td>\n      <td>C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>5512</th>\n      <td>HAM_0004921</td>\n      <td>ISIC_0025389</td>\n      <td>bkl</td>\n      <td>histo</td>\n      <td>65.0</td>\n      <td>male</td>\n      <td>neck</td>\n      <td>rosendahl</td>\n      <td>C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...</td>\n      <td>Benign keratosis-like lesions</td>\n      <td>2</td>\n      <td>unduplicated</td>\n    </tr>\n    <tr>\n      <th>5513</th>\n      <td>HAM_0002871</td>\n      <td>ISIC_0027044</td>\n      <td>df</td>\n      <td>consensus</td>\n      <td>35.0</td>\n      <td>female</td>\n      <td>lower extremity</td>\n      <td>vidir_molemax</td>\n      <td>C:/Users/User/Desktop/김세연/SKINCANCER_DRANSWER/...</td>\n      <td>Dermatofibroma</td>\n      <td>3</td>\n      <td>unduplicated</td>\n    </tr>\n  </tbody>\n</table>\n<p>5514 rows × 12 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "subject_map = pd.read_csv(base_dir+'HAM10000_subjectmap.csv')\n",
    "subject_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = subject_map.iloc[10]\n",
    "img_path = row['path']\n",
    "label = row['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(450, 600, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(img_path)\n",
    "print(np.array(image).shape)\n",
    "image = image.convert(\"RGB\")\n",
    "print(np.array(image).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "input_size = 224\n",
    "train_transform = transforms.Compose([transforms.Resize((input_size,input_size)),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.RandomVerticalFlip(),\n",
    "                                        transforms.RandomRotation(20),\n",
    "                                        transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n",
    "                                        transforms.ToTensor()])\n",
    "                                        #transforms.Normalize(normMean, normStd)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image = train_transform(image)"
   ]
  },
  {
   "source": [
    "image.shape"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_img_mean_std(image_paths):\n",
    "    \"\"\"\n",
    "        computing the mean and std of three channel on the whole dataset,\n",
    "        first we should normalize the image from 0-255 to 0-1\n",
    "    \"\"\"\n",
    "\n",
    "    img_h, img_w = 224, 224\n",
    "    imgs = []\n",
    "    means, stdevs = [], []\n",
    "\n",
    "    for i in tqdm(range(len(image_paths))):\n",
    "        img = cv2.imread(image_paths[i])\n",
    "        img = cv2.resize(img, (img_h, img_w))\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.stack(imgs, axis=3)\n",
    "    print(imgs.shape)\n",
    "\n",
    "    imgs = imgs.astype(np.float32) / 255.\n",
    "\n",
    "    for i in range(3):\n",
    "        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n",
    "        means.append(np.mean(pixels))\n",
    "        stdevs.append(np.std(pixels))\n",
    "\n",
    "    means.reverse()  # BGR --> RGB\n",
    "    stdevs.reverse()\n",
    "\n",
    "    print(\"normMean = {}\".format(means))\n",
    "    print(\"normStd = {}\".format(stdevs))\n",
    "    return means,stdevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/10015 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-377f86faf37c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcompute_img_mean_std\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_image_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-6c5eed80afae>\u001b[0m in \u001b[0;36mcompute_img_mean_std\u001b[1;34m(image_paths)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimg_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "compute_img_mean_std(all_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spiritual-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "respiratory-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "class BaseDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "    Base class for all data loaders\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, shuffle, validation_split, num_workers, collate_fn=default_collate):\n",
    "        self.validation_split = validation_split\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.batch_idx = 0\n",
    "        self.n_samples = len(dataset)\n",
    "\n",
    "        self.sampler, self.valid_sampler = self._split_sampler(self.validation_split)\n",
    "\n",
    "        self.init_kwargs = {\n",
    "            'dataset': dataset,\n",
    "            'batch_size': batch_size,\n",
    "            'shuffle': self.shuffle,\n",
    "            'collate_fn': collate_fn,\n",
    "            'num_workers': num_workers\n",
    "        }\n",
    "        super().__init__(sampler=self.sampler, **self.init_kwargs)\n",
    "\n",
    "    def _split_sampler(self, split):\n",
    "        if split == 0.0:\n",
    "            return None, None\n",
    "\n",
    "        idx_full = np.arange(self.n_samples)\n",
    "\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(idx_full)\n",
    "\n",
    "        if isinstance(split, int):\n",
    "            assert split > 0\n",
    "            assert split < self.n_samples, \"validation set size is configured to be larger than entire dataset.\"\n",
    "            len_valid = split\n",
    "        else:\n",
    "            len_valid = int(self.n_samples * split)\n",
    "\n",
    "        valid_idx = idx_full[0:len_valid]\n",
    "        train_idx = np.delete(idx_full, np.arange(0, len_valid))\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "        # turn off shuffle option which is mutually exclusive with sampler\n",
    "        self.shuffle = False\n",
    "        self.n_samples = len(train_idx)\n",
    "\n",
    "        return train_sampler, valid_sampler\n",
    "\n",
    "    def split_validation(self):\n",
    "        if self.valid_sampler is None:\n",
    "            return None\n",
    "        else:\n",
    "            return DataLoader(sampler=self.valid_sampler, **self.init_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unauthorized-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "### parse_config\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from functools import reduce, partial\n",
    "from operator import getitem\n",
    "from datetime import datetime\n",
    "from logger import setup_logging\n",
    "from utils import read_json, write_json\n",
    "\n",
    "\n",
    "class ConfigParser:\n",
    "    def __init__(self, config, resume=None, modification=None, run_id=None):\n",
    "        \"\"\"\n",
    "        class to parse configuration json file. Handles hyperparameters for training, initializations of modules, checkpoint saving\n",
    "        and logging module.\n",
    "        :param config: Dict containing configurations, hyperparameters for training. contents of `config.json` file for example.\n",
    "        :param resume: String, path to the checkpoint being loaded.\n",
    "        :param modification: Dict keychain:value, specifying position values to be replaced from config dict.\n",
    "        :param run_id: Unique Identifier for training processes. Used to save checkpoints and training log. Timestamp is being used as default\n",
    "        \"\"\"\n",
    "        # load config file and apply modification\n",
    "        self._config = _update_config(config, modification)\n",
    "        self.resume = resume\n",
    "\n",
    "        # set save_dir where trained model and log will be saved.\n",
    "        save_dir = Path(self.config['trainer']['save_dir'])\n",
    "\n",
    "        exper_name = self.config['name']\n",
    "        if run_id is None: # use timestamp as default run-id\n",
    "            run_id = datetime.now().strftime(r'%m%d_%H%M%S')\n",
    "        self._save_dir = save_dir / 'models' / exper_name / run_id\n",
    "        self._log_dir = save_dir / 'log' / exper_name / run_id\n",
    "\n",
    "        # make directory for saving checkpoints and log.\n",
    "        exist_ok = run_id == ''\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=exist_ok)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=exist_ok)\n",
    "\n",
    "        # save updated config file to the checkpoint dir\n",
    "        write_json(self.config, self.save_dir / 'config.json')\n",
    "\n",
    "        # configure logging module\n",
    "        setup_logging(self.log_dir)\n",
    "        self.log_levels = {\n",
    "            0: logging.WARNING,\n",
    "            1: logging.INFO,\n",
    "            2: logging.DEBUG\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_args(cls, args, options=''):\n",
    "        \"\"\"\n",
    "        Initialize this class from some cli arguments. Used in train, test.\n",
    "        \"\"\"\n",
    "        for opt in options:\n",
    "            args.add_argument(*opt.flags, default=None, type=opt.type)\n",
    "        if not isinstance(args, tuple):\n",
    "            args = args.parse_args()\n",
    "\n",
    "        if args.device is not None:\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device\n",
    "        if args.resume is not None:\n",
    "            resume = Path(args.resume)\n",
    "            cfg_fname = resume.parent / 'config.json'\n",
    "        else:\n",
    "            msg_no_cfg = \"Configuration file need to be specified. Add '-c config.json', for example.\"\n",
    "            assert args.config is not None, msg_no_cfg\n",
    "            resume = None\n",
    "            cfg_fname = Path(args.config)\n",
    "        \n",
    "        config = read_json(cfg_fname)\n",
    "        if args.config and resume:\n",
    "            # update new config for fine-tuning\n",
    "            config.update(read_json(args.config))\n",
    "\n",
    "        # parse custom cli options into dictionary\n",
    "        modification = {opt.target : getattr(args, _get_opt_name(opt.flags)) for opt in options}\n",
    "        return cls(config, resume, modification)\n",
    "\n",
    "    def init_obj(self, name, module, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Finds a function handle with the name given as 'type' in config, and returns the\n",
    "        instance initialized with corresponding arguments given.\n",
    "\n",
    "        `object = config.init_obj('name', module, a, b=1)`\n",
    "        is equivalent to\n",
    "        `object = module.name(a, b=1)`\n",
    "        \"\"\"\n",
    "        module_name = self[name]['type']\n",
    "        module_args = dict(self[name]['args'])\n",
    "        assert all([k not in module_args for k in kwargs]), 'Overwriting kwargs given in config file is not allowed'\n",
    "        module_args.update(kwargs)\n",
    "        return getattr(module, module_name)(*args, **module_args)\n",
    "\n",
    "    def init_ftn(self, name, module, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Finds a function handle with the name given as 'type' in config, and returns the\n",
    "        function with given arguments fixed with functools.partial.\n",
    "\n",
    "        `function = config.init_ftn('name', module, a, b=1)`\n",
    "        is equivalent to\n",
    "        `function = lambda *args, **kwargs: module.name(a, *args, b=1, **kwargs)`.\n",
    "        \"\"\"\n",
    "        module_name = self[name]['type']\n",
    "        module_args = dict(self[name]['args'])\n",
    "        assert all([k not in module_args for k in kwargs]), 'Overwriting kwargs given in config file is not allowed'\n",
    "        module_args.update(kwargs)\n",
    "        return partial(getattr(module, module_name), *args, **module_args)\n",
    "\n",
    "    def __getitem__(self, name):\n",
    "        \"\"\"Access items like ordinary dict.\"\"\"\n",
    "        return self.config[name]\n",
    "\n",
    "    def get_logger(self, name, verbosity=2):\n",
    "        msg_verbosity = 'verbosity option {} is invalid. Valid options are {}.'.format(verbosity, self.log_levels.keys())\n",
    "        assert verbosity in self.log_levels, msg_verbosity\n",
    "        logger = logging.getLogger(name)\n",
    "        logger.setLevel(self.log_levels[verbosity])\n",
    "        return logger\n",
    "\n",
    "    # setting read-only attributes\n",
    "    @property\n",
    "    def config(self):\n",
    "        return self._config\n",
    "\n",
    "    @property\n",
    "    def save_dir(self):\n",
    "        return self._save_dir\n",
    "\n",
    "    @property\n",
    "    def log_dir(self):\n",
    "        return self._log_dir\n",
    "\n",
    "# helper functions to update config dict with custom cli options\n",
    "def _update_config(config, modification):\n",
    "    if modification is None:\n",
    "        return config\n",
    "\n",
    "    for k, v in modification.items():\n",
    "        if v is not None:\n",
    "            _set_by_path(config, k, v)\n",
    "    return config\n",
    "\n",
    "def _get_opt_name(flags):\n",
    "    for flg in flags:\n",
    "        if flg.startswith('--'):\n",
    "            return flg.replace('--', '')\n",
    "    return flags[0].replace('--', '')\n",
    "\n",
    "def _set_by_path(tree, keys, value):\n",
    "    \"\"\"Set a value in a nested object in tree by sequence of keys.\"\"\"\n",
    "    keys = keys.split(';')\n",
    "    _get_by_path(tree, keys[:-1])[keys[-1]] = value\n",
    "\n",
    "def _get_by_path(tree, keys):\n",
    "    \"\"\"Access a nested object in tree by sequence of keys.\"\"\"\n",
    "    return reduce(getitem, keys, tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lasting-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MnistDataLoader(BaseDataLoader):\n",
    "    \"\"\"\n",
    "    MNIST data loading demo using BaseDataLoader\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, batch_size, shuffle=True, validation_split=0.0, num_workers=1, training=True):\n",
    "        trsfm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset = datasets.MNIST(self.data_dir, train=training, download=True, transform=trsfm)\n",
    "        super().__init__(self.dataset, batch_size, shuffle, validation_split, num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "treated-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"name\": \"Mnist_LeNet\",\n",
    "    \"n_gpu\": 1,\n",
    "\n",
    "    \"arch\": {\n",
    "        \"type\": \"MnistModel\",\n",
    "        \"args\": {}\n",
    "    },\n",
    "    \"data_loader\": {\n",
    "        \"type\": \"MnistDataLoader\",\n",
    "        \"args\":{\n",
    "            \"data_dir\": \"data/\",\n",
    "            \"batch_size\": 128,\n",
    "            \"shuffle\": True,\n",
    "            \"validation_split\": 0.1,\n",
    "            \"num_workers\": 2\n",
    "        }\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"args\":{\n",
    "            \"lr\": 0.001,\n",
    "            \"weight_decay\": 0,\n",
    "            \"amsgrad\": True\n",
    "        }\n",
    "    },\n",
    "    \"loss\": \"nll_loss\",\n",
    "    \"metrics\": [\n",
    "        \"accuracy\", \"top_k_acc\"\n",
    "    ],\n",
    "    \"lr_scheduler\": {\n",
    "        \"type\": \"StepLR\",\n",
    "        \"args\": {\n",
    "            \"step_size\": 50,\n",
    "            \"gamma\": 0.1\n",
    "        }\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"epochs\": 100,\n",
    "\n",
    "        \"save_dir\": \"saved/\",\n",
    "        \"save_period\": 1,\n",
    "        \"verbosity\": 2,\n",
    "        \n",
    "        \"monitor\": \"min val_loss\",\n",
    "        \"early_stop\": 10,\n",
    "\n",
    "        \"tensorboard\": True\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "HTTPError",
     "evalue": "HTTP Error 503: Service Unavailable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c8d17a0eadf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1307\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0.3081\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         ])\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrsfm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmd5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;31m# process and save as torch files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mdownload_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[0marchive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0m_urlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m     \u001b[1;31m# check integrity of downloaded file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Downloading '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0m_urlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'https'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[1;34m(url, filename, chunk_size)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_urlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"User-Agent\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 641\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 503: Service Unavailable"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "trsfm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "datasets.MNIST('./data/', train=True, download=True, transform=trsfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "suited-verification",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'init_obj'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e0c1732fd669>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_obj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data_loader'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'init_obj'"
     ]
    }
   ],
   "source": [
    "data_loader = config.init_obj('data_loader', module_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "herbal-ebony",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'wget'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n",
      "tar: Error opening archive: Failed to open 'MNIST.tar.gz'\n"
     ]
    }
   ],
   "source": [
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}